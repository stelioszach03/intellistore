{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IntelliStore ML: Data Collection and Preprocessing\n",
    "\n",
    "This notebook demonstrates how to collect and preprocess access log data from Kafka for training the hot/cold tiering model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from kafka import KafkaConsumer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Kafka Consumer Setup\n",
    "\n",
    "Connect to Kafka and consume access logs from the `access-logs` topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kafka configuration\n",
    "KAFKA_BROKERS = ['localhost:9092']  # Update for your environment\n",
    "ACCESS_LOGS_TOPIC = 'access-logs'\n",
    "GROUP_ID = 'ml-data-collection'\n",
    "\n",
    "def create_kafka_consumer():\n",
    "    \"\"\"Create and configure Kafka consumer\"\"\"\n",
    "    consumer = KafkaConsumer(\n",
    "        ACCESS_LOGS_TOPIC,\n",
    "        bootstrap_servers=KAFKA_BROKERS,\n",
    "        group_id=GROUP_ID,\n",
    "        value_deserializer=lambda m: json.loads(m.decode('utf-8')),\n",
    "        auto_offset_reset='earliest',\n",
    "        enable_auto_commit=True,\n",
    "        consumer_timeout_ms=10000  # 10 second timeout\n",
    "    )\n",
    "    return consumer\n",
    "\n",
    "print(\"Kafka consumer configuration ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Collection\n",
    "\n",
    "Collect access logs and convert them to a structured format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_access_logs(max_messages=1000):\n",
    "    \"\"\"Collect access logs from Kafka\"\"\"\n",
    "    consumer = create_kafka_consumer()\n",
    "    messages = []\n",
    "    \n",
    "    print(f\"Collecting up to {max_messages} messages...\")\n",
    "    \n",
    "    try:\n",
    "        for message in consumer:\n",
    "            messages.append(message.value)\n",
    "            \n",
    "            if len(messages) >= max_messages:\n",
    "                break\n",
    "                \n",
    "            if len(messages) % 100 == 0:\n",
    "                print(f\"Collected {len(messages)} messages...\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Collection stopped: {e}\")\n",
    "    \n",
    "    finally:\n",
    "        consumer.close()\n",
    "    \n",
    "    print(f\"Collected {len(messages)} total messages\")\n",
    "    return messages\n",
    "\n",
    "# For demo purposes, let's create synthetic data\n",
    "def generate_synthetic_access_logs(num_records=5000):\n",
    "    \"\"\"Generate synthetic access logs for demonstration\"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    users = [f\"user_{i}\" for i in range(1, 101)]  # 100 users\n",
    "    buckets = [f\"bucket_{i}\" for i in range(1, 21)]  # 20 buckets\n",
    "    actions = ['upload_object', 'download_object', 'delete_object']\n",
    "    content_types = ['image/jpeg', 'video/mp4', 'application/pdf', 'text/plain', 'application/zip']\n",
    "    \n",
    "    logs = []\n",
    "    base_time = time.time() - (7 * 24 * 3600)  # 7 days ago\n",
    "    \n",
    "    for i in range(num_records):\n",
    "        # Generate timestamp (more recent = higher probability)\n",
    "        timestamp = base_time + np.random.exponential(scale=2 * 24 * 3600)\n",
    "        \n",
    "        # Generate object key\n",
    "        object_key = f\"object_{np.random.randint(1, 1001)}.{np.random.choice(['jpg', 'mp4', 'pdf', 'txt', 'zip'])}\"\n",
    "        \n",
    "        # Generate size (log-normal distribution)\n",
    "        size = int(np.random.lognormal(mean=15, sigma=2))  # ~3MB average\n",
    "        \n",
    "        # Generate tier (80% hot, 20% cold initially)\n",
    "        tier = np.random.choice(['hot', 'cold'], p=[0.8, 0.2])\n",
    "        \n",
    "        log_entry = {\n",
    "            'timestamp': timestamp,\n",
    "            'user': np.random.choice(users),\n",
    "            'action': np.random.choice(actions, p=[0.3, 0.6, 0.1]),  # More downloads\n",
    "            'bucket': np.random.choice(buckets),\n",
    "            'object': object_key,\n",
    "            'size': size,\n",
    "            'tier': tier,\n",
    "            'success': np.random.choice([True, False], p=[0.95, 0.05]),\n",
    "            'metadata': {\n",
    "                'content_type': np.random.choice(content_types),\n",
    "                'user_agent': 'intellistore-client/1.0'\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        logs.append(log_entry)\n",
    "    \n",
    "    return logs\n",
    "\n",
    "# Generate synthetic data for demo\n",
    "print(\"Generating synthetic access logs...\")\n",
    "access_logs = generate_synthetic_access_logs(5000)\n",
    "print(f\"Generated {len(access_logs)} synthetic access logs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing\n",
    "\n",
    "Convert raw access logs into a structured DataFrame with features for ML training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_access_logs(logs: List[Dict]) -> pd.DataFrame:\n",
    "    \"\"\"Convert access logs to DataFrame and add features\"\"\"\n",
    "    df = pd.DataFrame(logs)\n",
    "    \n",
    "    # Convert timestamp to datetime\n",
    "    df['datetime'] = pd.to_datetime(df['timestamp'], unit='s')\n",
    "    \n",
    "    # Extract time-based features\n",
    "    df['hour_of_day'] = df['datetime'].dt.hour\n",
    "    df['day_of_week'] = df['datetime'].dt.dayofweek\n",
    "    df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\n",
    "    df['is_business_hours'] = ((df['hour_of_day'] >= 9) & (df['hour_of_day'] <= 17)).astype(int)\n",
    "    \n",
    "    # Object age (days since first access)\n",
    "    first_access = df.groupby('object')['datetime'].min().to_dict()\n",
    "    df['object_age_days'] = (df['datetime'] - df['object'].map(first_access)).dt.total_seconds() / (24 * 3600)\n",
    "    \n",
    "    # Size categories\n",
    "    df['size_category'] = pd.cut(df['size'], \n",
    "                                bins=[0, 1024, 1024*1024, 100*1024*1024, float('inf')],\n",
    "                                labels=['small', 'medium', 'large', 'xlarge'])\n",
    "    \n",
    "    # Content type categories\n",
    "    df['content_type'] = df['metadata'].apply(lambda x: x.get('content_type', 'unknown'))\n",
    "    df['is_media'] = df['content_type'].str.startswith(('image/', 'video/', 'audio/')).astype(int)\n",
    "    \n",
    "    # User activity features\n",
    "    user_activity = df.groupby('user').size().to_dict()\n",
    "    df['user_activity_level'] = df['user'].map(user_activity)\n",
    "    \n",
    "    # Bucket popularity\n",
    "    bucket_popularity = df.groupby('bucket').size().to_dict()\n",
    "    df['bucket_popularity'] = df['bucket'].map(bucket_popularity)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Preprocess the data\n",
    "print(\"Preprocessing access logs...\")\n",
    "df = preprocess_access_logs(access_logs)\n",
    "print(f\"Preprocessed DataFrame shape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering\n",
    "\n",
    "Create features that will help predict whether an object will be \"hot\" (frequently accessed) in the next 24 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ml_features(df: pd.DataFrame, prediction_window_hours=24) -> pd.DataFrame:\n",
    "    \"\"\"Create features for ML model training\"\"\"\n",
    "    # Sort by timestamp\n",
    "    df = df.sort_values('datetime').reset_index(drop=True)\n",
    "    \n",
    "    # Create sliding window features for each object\n",
    "    features_list = []\n",
    "    \n",
    "    # Group by object to calculate per-object features\n",
    "    for object_key, object_df in df.groupby('object'):\n",
    "        object_df = object_df.sort_values('datetime').reset_index(drop=True)\n",
    "        \n",
    "        for i in range(len(object_df)):\n",
    "            current_time = object_df.iloc[i]['datetime']\n",
    "            \n",
    "            # Look back window (past 7 days)\n",
    "            lookback_start = current_time - timedelta(days=7)\n",
    "            lookback_data = object_df[object_df['datetime'].between(lookback_start, current_time)]\n",
    "            \n",
    "            # Look forward window (next 24 hours) for label\n",
    "            lookahead_end = current_time + timedelta(hours=prediction_window_hours)\n",
    "            lookahead_data = object_df[object_df['datetime'].between(current_time, lookahead_end)]\n",
    "            \n",
    "            # Skip if not enough future data for labeling\n",
    "            if lookahead_data.empty:\n",
    "                continue\n",
    "            \n",
    "            # Calculate features\n",
    "            features = {\n",
    "                'object_key': object_key,\n",
    "                'timestamp': current_time,\n",
    "                'hour_of_day': object_df.iloc[i]['hour_of_day'],\n",
    "                'day_of_week': object_df.iloc[i]['day_of_week'],\n",
    "                'is_weekend': object_df.iloc[i]['is_weekend'],\n",
    "                'is_business_hours': object_df.iloc[i]['is_business_hours'],\n",
    "                'object_age_days': object_df.iloc[i]['object_age_days'],\n",
    "                'size': object_df.iloc[i]['size'],\n",
    "                'size_category': object_df.iloc[i]['size_category'],\n",
    "                'is_media': object_df.iloc[i]['is_media'],\n",
    "                'current_tier': object_df.iloc[i]['tier'],\n",
    "                'user_activity_level': object_df.iloc[i]['user_activity_level'],\n",
    "                'bucket_popularity': object_df.iloc[i]['bucket_popularity'],\n",
    "                \n",
    "                # Historical access patterns (past 7 days)\n",
    "                'access_count_7d': len(lookback_data),\n",
    "                'download_count_7d': len(lookback_data[lookback_data['action'] == 'download_object']),\n",
    "                'unique_users_7d': lookback_data['user'].nunique(),\n",
    "                'avg_daily_access': len(lookback_data) / 7,\n",
    "                'last_access_hours_ago': (current_time - lookback_data['datetime'].max()).total_seconds() / 3600 if not lookback_data.empty else 999,\n",
    "                \n",
    "                # Recent trend (past 24 hours vs past 7 days)\n",
    "                'recent_access_trend': len(lookback_data[lookback_data['datetime'] >= current_time - timedelta(hours=24)]) / max(1, len(lookback_data) / 7),\n",
    "            }\n",
    "            \n",
    "            # Create label: will this object be accessed > threshold times in next 24 hours?\n",
    "            future_access_count = len(lookahead_data[lookahead_data['action'] == 'download_object'])\n",
    "            features['future_access_count'] = future_access_count\n",
    "            features['is_hot'] = int(future_access_count >= 3)  # Threshold: 3+ accesses = hot\n",
    "            \n",
    "            features_list.append(features)\n",
    "    \n",
    "    return pd.DataFrame(features_list)\n",
    "\n",
    "# Create ML features\n",
    "print(\"Creating ML features...\")\n",
    "ml_df = create_ml_features(df)\n",
    "print(f\"ML DataFrame shape: {ml_df.shape}\")\n",
    "print(f\"Hot objects: {ml_df['is_hot'].sum()} ({ml_df['is_hot'].mean():.2%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Exploratory Data Analysis\n",
    "\n",
    "Analyze the data to understand patterns and relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics\n",
    "print(\"=== Dataset Overview ===\")\n",
    "print(f\"Total samples: {len(ml_df)}\")\n",
    "print(f\"Hot objects: {ml_df['is_hot'].sum()} ({ml_df['is_hot'].mean():.2%})\")\n",
    "print(f\"Cold objects: {(~ml_df['is_hot'].astype(bool)).sum()} ({(~ml_df['is_hot'].astype(bool)).mean():.2%})\")\n",
    "print(f\"Date range: {ml_df['timestamp'].min()} to {ml_df['timestamp'].max()}\")\n",
    "\n",
    "# Feature distributions\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "fig.suptitle('Feature Distributions', fontsize=16)\n",
    "\n",
    "# Hour of day distribution\n",
    "ml_df.groupby(['hour_of_day', 'is_hot']).size().unstack().plot(kind='bar', ax=axes[0,0])\n",
    "axes[0,0].set_title('Access Patterns by Hour')\n",
    "axes[0,0].set_xlabel('Hour of Day')\n",
    "axes[0,0].legend(['Cold', 'Hot'])\n",
    "\n",
    "# Day of week distribution\n",
    "ml_df.groupby(['day_of_week', 'is_hot']).size().unstack().plot(kind='bar', ax=axes[0,1])\n",
    "axes[0,1].set_title('Access Patterns by Day of Week')\n",
    "axes[0,1].set_xlabel('Day of Week (0=Monday)')\n",
    "axes[0,1].legend(['Cold', 'Hot'])\n",
    "\n",
    "# Object age distribution\n",
    "ml_df.boxplot(column='object_age_days', by='is_hot', ax=axes[0,2])\n",
    "axes[0,2].set_title('Object Age Distribution')\n",
    "axes[0,2].set_xlabel('Is Hot')\n",
    "\n",
    "# Size distribution\n",
    "ml_df.boxplot(column='size', by='is_hot', ax=axes[1,0])\n",
    "axes[1,0].set_title('Object Size Distribution')\n",
    "axes[1,0].set_xlabel('Is Hot')\n",
    "axes[1,0].set_yscale('log')\n",
    "\n",
    "# Access count distribution\n",
    "ml_df.boxplot(column='access_count_7d', by='is_hot', ax=axes[1,1])\n",
    "axes[1,1].set_title('7-Day Access Count Distribution')\n",
    "axes[1,1].set_xlabel('Is Hot')\n",
    "\n",
    "# Recent trend distribution\n",
    "ml_df.boxplot(column='recent_access_trend', by='is_hot', ax=axes[1,2])\n",
    "axes[1,2].set_title('Recent Access Trend Distribution')\n",
    "axes[1,2].set_xlabel('Is Hot')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Correlation matrix\n",
    "numeric_cols = ml_df.select_dtypes(include=[np.number]).columns\n",
    "correlation_matrix = ml_df[numeric_cols].corr()\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, fmt='.2f')\n",
    "plt.title('Feature Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Export\n",
    "\n",
    "Save the processed data for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed data\n",
    "output_file = '../data/processed_access_logs.csv'\n",
    "ml_df.to_csv(output_file, index=False)\n",
    "print(f\"Saved processed data to {output_file}\")\n",
    "\n",
    "# Save feature metadata\n",
    "feature_info = {\n",
    "    'total_samples': len(ml_df),\n",
    "    'hot_samples': ml_df['is_hot'].sum(),\n",
    "    'cold_samples': (~ml_df['is_hot'].astype(bool)).sum(),\n",
    "    'features': list(ml_df.columns),\n",
    "    'numeric_features': list(ml_df.select_dtypes(include=[np.number]).columns),\n",
    "    'categorical_features': list(ml_df.select_dtypes(include=['object', 'category']).columns),\n",
    "    'target_column': 'is_hot',\n",
    "    'date_range': {\n",
    "        'start': str(ml_df['timestamp'].min()),\n",
    "        'end': str(ml_df['timestamp'].max())\n",
    "    }\n",
    "}\n",
    "\n",
    "import json\n",
    "with open('../data/feature_metadata.json', 'w') as f:\n",
    "    json.dump(feature_info, f, indent=2, default=str)\n",
    "\n",
    "print(\"Data collection and preprocessing completed!\")\n",
    "print(f\"Next step: Run the model training notebook with {len(ml_df)} samples\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}